1. environment.py 함수들 문제점 및 수정사항
  - 그리드월드 가로 세로 수정
  - build canvas : 캔버스에 이미지 추가(agent, hazard, destination 위치 적용)
  - 그리드 map 가치함수와 행동확률 출력을 위해 text_value, print_value_q_all 추가
  - step 함수 문제에 맞게 보상 적용
    현재 상태의 가치함수도 같이 반환
  - 완료.

2. mc_agent.py 함수들 문제점 및 수정사항
  - 생성자 : Grid World 너비 및 높이 수정 행동 확률, 보상 table 추가
  - 보상 table을 위해 save_reward 함수 추가(main 함수에서 사용)
  - 기존 update : G 값을 구하기 위해 reversed 된 state를 first visit의 판단으로 사용함
    사실상 last visit을 사용하고 있음. -> first visit 을 하도록 수정
    first visit을 하도록 수정하여, G를 구할 때 역순으로 계산이 안되는 문제점이 생기므로,
    G를 역순으로 계산 하는 반복문을 추가함.
    전체적인 코드 수정이므로 박스 미사용 / 코드 다시 캡쳐해서 진행
  - get_action : 큐 함수에 따라서 행동을 반환하고, 입실론 탐욕 정책에 따라서 행동을 반환함.
    큐함수 값 변수를 추가함. 그러나 기존 코드에서의 possible_next_state는 큐함수를 반환하지 않음.(큐함수를 구하는 부분이 부재되어있음)
    행동 확률 값을 화면에 출력하기 위한 코드 추가
    possible_next_state : 기존 코드 : 가치함수 만을 사용하며, 감가율과 보상이 적용되어있지않음. 가치함수와 보상, 감가율 모두 적용 가능하도록 수정
    next_state를 next_state_value로 변경, next_state_reward 추가 -> 각 행동들에 대한 다음 상태의 가치함수와 보상을 계산하여 저장함.
    큐 함수 값을 계산하는 코드 추가, 다음 상태의 가치함수 값과 큐함수 값 모두를 반환하도록 하였음.

  - get_reward 추가 : 각 상태에 대한 보상을 반환함. 해저드에 지정된 지역은 보상이 -1 목표지점은 1, 그외는 0으로 설정하였음.
    
3. main 문제점 및 수정 사항
  - 문제점 및 수정 사항 : 현재 상태에 대한 가치함수 반환 부분 부재 -> present_state 를 얻어오고, 샘플에 저장
    모든 큐함수와 가치함수를 화면에 표시하도록 print_value_q_all 함수 호출
    에피소드 완료 시마다 hazard 방문 횟수와 목표 지점 방문 횟수를 계산하고, 에피소드 횟수, 장애물 및 목표지점 방문 횟수를 관측하기 위해 출력하는 코드 추가    
    